{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from copy import deepcopy\n",
    "\n",
    "from keras.layers import Embedding, Dense, LSTM, SimpleRNN, Flatten, Dropout, Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up glove\n",
    "glove_dir = '/Users/ericrostedt/Desktop/Python/Till채mpad_Maskininl채rning/Labb4/glove.6B.100d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(glove_dir)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].lower()\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_index['sweden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate closest words\n",
    "def cosine_similarity(v1,v2):\n",
    "    return v1@v2/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    \n",
    "def get_5_closest_words(word):\n",
    "    closest_words = []\n",
    "    closest_dist = []\n",
    "    v1 = np.array(embeddings_index[word])\n",
    "    for k,v in embeddings_index.items():\n",
    "        if k == word:\n",
    "            continue\n",
    "        v2 = np.array(v)\n",
    "        dist = cosine_similarity(v1,v2)\n",
    "        for i in range(5):\n",
    "            if i >= len(closest_words):\n",
    "                closest_words.append(k)\n",
    "                closest_dist.append(dist)\n",
    "                break\n",
    "            w, val = closest_words[i],closest_dist[i]\n",
    "            if dist>val:\n",
    "                closest_words.insert(i,k)\n",
    "                closest_dist.insert(i,dist)\n",
    "                if len(closest_words) > 5:\n",
    "                    del closest_words[-1]\n",
    "                break\n",
    "            \n",
    "    return closest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test closests words\n",
    "words = ['table','france','sweden']\n",
    "for w in words:\n",
    "    closest = get_5_closest_words(w)\n",
    "    print(\"Closest words to %s are %s, %s, %s, %s, %s\" %(w,*closest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "BASE_DIR = '/Users/ericrostedt/Desktop/Python/Till채mpad_Maskininl채rning/Labb4/NER-data/'\n",
    "\n",
    "def load_conll2003_en():\n",
    "    train_file = BASE_DIR + 'eng.train'\n",
    "    dev_file = BASE_DIR + 'eng.valid'\n",
    "    test_file = BASE_DIR + 'eng.test'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to transform data\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create sequences\n",
    "def build_sequences(corpus_dict, key_x='form', key_y='pos', tolower=True):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = [word[key_x] for word in sentence]\n",
    "        y = [word[key_y] for word in sentence]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct sets\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "X_train_cat, Y_train_cat = build_sequences(train_dict, key_y='ner')\n",
    "\n",
    "temp_word_list = deepcopy(X_train_cat)\n",
    "temp_word_list.extend([embeddings_index.keys()])\n",
    "\n",
    "word_set = sorted(list(set([item for sublist in temp_word_list for item in sublist])))\n",
    "ner_set = sorted(list(set([item for sublist in Y_train_cat for item in sublist])))\n",
    "print(len(word_set))\n",
    "print(len(ner_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct mapping between words and indicies.\n",
    "rev_word_idx = dict(enumerate(word_set, start=2))\n",
    "rev_ner_idx = dict(enumerate(ner_set, start=2))\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "ner_idx = {v: k for k, v in rev_ner_idx.items()}\n",
    "\n",
    "print(word_idx[\"sweden\"])\n",
    "print(rev_word_idx[351800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct uniform starting matrix\n",
    "m, n = len(word_set)+2, len(embeddings_index[\"sweden\"])\n",
    "matrix = np.random.rand(m,n) - 1/2\n",
    "print(np.shape(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert glove information\n",
    "for key, value in embeddings_index.items():\n",
    "    index = word_idx[key]\n",
    "    matrix[index] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class \n",
    "def to_index(X, idx):\n",
    "    \"\"\"\n",
    "    Convert the word lists (or POS lists) to indexes\n",
    "    :param X: List of word (or POS) lists\n",
    "    :param idx: word to number dictionary\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_idx = []\n",
    "    for x in X:\n",
    "        # We map the unknown words to one\n",
    "        x_idx = list(map(lambda x: idx.get(x, 1), x))\n",
    "        X_idx += [x_idx]\n",
    "    return X_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, In X_dict, we replace the words with their index\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_train_idx = to_index(X_train_cat, word_idx)\n",
    "Y_train_idx = to_index(Y_train_cat, ner_idx)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_idx, maxlen=150)\n",
    "Y_train_padded = pad_sequences(Y_train_idx, maxlen=150)\n",
    "\n",
    "# One extra symbol for 0 (padding)\n",
    "Y_train_padded_vectorized = to_categorical(Y_train_padded, \n",
    "                                          num_classes=len(ner_set) + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev, In X_dict, we replace the words with their index\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "X_dev_cat, Y_dev_cat = build_sequences(dev_dict, key_y='ner')\n",
    "# We create the parallel sequences of indexes\n",
    "X_dev_idx = to_index(X_dev_cat, word_idx)\n",
    "Y_dev_idx = to_index(Y_dev_cat, ner_idx)\n",
    "\n",
    "X_dev_padded = pad_sequences(X_dev_idx, maxlen=150)\n",
    "Y_dev_padded = pad_sequences(Y_dev_idx, maxlen=150)\n",
    "\n",
    "# One extra symbol for 0 (padding)\n",
    "Y_dev_padded_vectorized = to_categorical(Y_dev_padded, \n",
    "                                          num_classes=len(ner_set) + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In X_dict, we replace the words with their index\n",
    "test_dict = conll_dict.transform(test_sentences)\n",
    "X_test_cat, Y_test_cat = build_sequences(test_dict, key_y='ner')\n",
    "# We create the parallel sequences of indexes\n",
    "X_test_idx = to_index(X_test_cat, word_idx)\n",
    "Y_test_idx = to_index(Y_test_cat, ner_idx)\n",
    "\n",
    "X_test_padded = pad_sequences(X_test_idx, maxlen=150)\n",
    "Y_test_padded = pad_sequences(Y_test_idx, maxlen=150)\n",
    "\n",
    "# One extra symbol for 0 (padding)\n",
    "Y_test_padded_vectorized = to_categorical(Y_test_padded, \n",
    "                                          num_classes=len(ner_set) + 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(m,\n",
    "                    100,\n",
    "                    mask_zero=True, input_length=150, trainable=False))\n",
    "model.add(SimpleRNN(32, activation='relu', return_sequences=True))\n",
    "model.add(Dense(len(ner_set) + 2, activation='softmax'))\n",
    "model.layers[0].set_weights([matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "history = model.fit(X_train_padded, Y_train_padded_vectorized,\n",
    "                  epochs=30, batch_size=32, validation_data=(X_dev_padded, Y_dev_padded_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test_padded, Y_test_padded_vectorized)\n",
    "print('Test acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build model and compile LSTM model\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(Embedding(m,\n",
    "                    100,\n",
    "                    mask_zero=True, input_length=150, trainable=False))\n",
    "model_LSTM.add(Dropout(0.3))\n",
    "model_LSTM.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True)))\n",
    "model_LSTM.add(Dropout(0.3))\n",
    "model_LSTM.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "model_LSTM.add(Dropout(0.5))\n",
    "model_LSTM.add(Dense(len(ner_set) + 2, activation='softmax'))\n",
    "\n",
    "model_LSTM.layers[0].set_weights([matrix])\n",
    "\n",
    "model_LSTM.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM.load_weights('weights_LSTM_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "model_LSTM.fit(X_train_padded, Y_train_padded_vectorized,\n",
    "                  epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM.save_weights('weights_LSTM_final1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model_LSTM.evaluate(X_test_padded,  Y_test_padded_vectorized)\n",
    "print('Test acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "corpus_pos_predictions = model_LSTM.predict(X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pred_num = []\n",
    "for sent_nbr, sent_pos_predictions in enumerate(corpus_pos_predictions):\n",
    "    pos_pred_num += [sent_pos_predictions[-len(X_test_cat[sent_nbr]):]]\n",
    "print(pos_pred_num[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pred = []\n",
    "for sentence in pos_pred_num:\n",
    "    pos_pred_idx = list(map(np.argmax, sentence))\n",
    "    pos_pred_cat = list(map(rev_ner_idx.get, pos_pred_idx))\n",
    "    pos_pred += [pos_pred_cat]\n",
    "\n",
    "print(pos_pred[:2])\n",
    "print(X_test_cat[:2])\n",
    "print(Y_test_cat[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total, correct, total_ukn, correct_ukn = 0, 0, 0, 0\n",
    "for id_s, sentence in enumerate(X_test_cat):\n",
    "    for id_w, word in enumerate(sentence):\n",
    "        total += 1\n",
    "        if pos_pred[id_s][id_w] == Y_test_cat[id_s][id_w]:\n",
    "            correct += 1\n",
    "        # The word is not in the dictionary\n",
    "        if word not in word_idx:\n",
    "            total_ukn += 1\n",
    "            if pos_pred[id_s][id_w] == Y_test_cat[id_s][id_w]:\n",
    "                correct_ukn += 1\n",
    "\n",
    "print('total %d, correct %d, accuracy %f' % \n",
    "      (total, correct, correct / total))\n",
    "if total_ukn != 0:\n",
    "    print('total unknown %d, correct %d, accuracy %f' % \n",
    "          (total_ukn, correct_ukn, correct_ukn / total_ukn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence, model, word_idx, \n",
    "                     vocabulary_words, idx_pos, verbose=False):\n",
    "    # Predict one sentence\n",
    "    sentence = sentence.split()\n",
    "    len_sentence = len(sentence)\n",
    "    word_idxs = to_index([sentence], word_idx)\n",
    "    word_idxs = pad_sequences(word_idxs,maxlen=150)\n",
    "\n",
    "    pos_idx_pred = model.predict(word_idxs)\n",
    "    pos_idxs = [np.argmax(x) for x in pos_idx_pred[0]]\n",
    "    pos_idxs = pos_idxs[-len_sentence:]\n",
    "    pos = list(map(idx_pos.get, pos_idxs))\n",
    "    if verbose:\n",
    "        print('Sentence', sentence)\n",
    "        print('Sentence word indexes', word_idxs)\n",
    "        #print('Padded sentence', word_idx_padded)\n",
    "        print('POS predicted', pos_idx_pred[0])\n",
    "        print('POS shape', pos_idx_pred.shape)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_test_cat = []\n",
    "for sentence in X_test_cat:\n",
    "    new_sentence = \"\"\n",
    "    for w in sentence:\n",
    "        new_sentence += w + \" \"\n",
    "    new_X_test_cat.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_cat = []\n",
    "for sentence in new_X_test_cat:\n",
    "    y_test_pred_cat.append(predict_sentence(sentence.lower(), \n",
    "                                       model_LSTM, word_idx, \n",
    "                                       word_set, \n",
    "                                       rev_ner_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_cat[1])\n",
    "print(new_X_test_cat[1])\n",
    "print(y_test_pred_cat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"results.txt\", \"w\")\n",
    "for i, prediction in enumerate(y_test_pred_cat):\n",
    "    for j, pred_word in enumerate(prediction):\n",
    "        file.write(Y_test_cat[i][j] + \" \" + str(pred_word) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.argmax(conf_mat.T[i]) == np.argmax(conf_mat[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
